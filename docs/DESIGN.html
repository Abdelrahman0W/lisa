
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Technical Specification Document &#8212; LISA 0.1.0 documentation</title>
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Contributing Guidelines" href="CONTRIBUTING.html" />
    <link rel="prev" title="How to Use Pytest and LISA" href="USAGE.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="technical-specification-document">
<h1>Technical Specification Document<a class="headerlink" href="#technical-specification-document" title="Permalink to this headline">¶</a></h1>
<p>This document outlines the technical specifications and design for
LISAv3 leveraging <a class="reference external" href="https://docs.pytest.org/en/stable/">Pytest</a> as
the test runner.</p>
<p>Please see <a class="reference external" href="https://github.com/microsoft/lisa/pull/1107">PR #1107</a>
for a working implementation and see the <a class="reference external" href="https://microsoft.github.io/lisa/.">documentation</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Author</dt>
<dd class="field-odd"><p>Andrew Schwartzmeyer (he/him) &lt;<a class="reference external" href="mailto:andrew&#37;&#52;&#48;schwartzmeyer&#46;com">andrew<span>&#64;</span>schwartzmeyer<span>&#46;</span>com</a>&gt;</p>
</dd>
<dt class="field-even">Version</dt>
<dd class="field-even"><p>0.4</p>
</dd>
</dl>
<div class="section" id="why-pytest">
<h2>Why Pytest?<a class="headerlink" href="#why-pytest" title="Permalink to this headline">¶</a></h2>
<p>Pytest is an <a class="reference external" href="https://docs.pytest.org/en/stable/talks.html">incredibly popular</a> MIT licensed open
source Python testing framework. It has a thriving community and
plugin framework, with over 750 <a class="reference external" href="https://plugincompat.herokuapp.com/">plugins</a>. Instead of writing (and
therefore maintaining) yet another test framework, we will do more
with less by reusing Pytest and existing plugins. This allows us to
focus on our unique problems: organizing and understanding our tests,
deploying necessary resources (such as Azure, Hyper-V, or bare metal
machines, collectively known as “targets”), and analyzing our results.</p>
<p>Most of Pytest itself is implemented via <a class="reference external" href="https://docs.pytest.org/en/stable/plugins.html">built-in plugins</a>, providing us with
many useful and well-documented examples. Furthermore, when others
were confronted with a problem similar to our own they also chose to
use Pytest.</p>
<p><a class="reference external" href="https://github.com/labgrid-project/labgrid">Labgrid</a> is an open source embedded board control library that
delegated the testing framework logic to Pytest in their <a class="reference external" href="https://labgrid.readthedocs.io/en/latest/design_decisions.html">design</a>,
and <a class="reference external" href="https://github.com/u-boot/u-boot">U-Boot</a>, an embedded board
boot loader, similarly leveraged Pytest in their <a class="reference external" href="https://github.com/u-boot/u-boot/tree/master/test/py">tests</a>. KernelCI and
Avocado were also evaluated by the Labgrid developers at an <a class="reference external" href="https://youtu.be/S0EJJM5bVUY">Embedded
Linux Conference</a> and both ruled out
for reasons similar to our own before they settled on Pytest.</p>
<p>The <a class="reference external" href="https://youtu.be/CMuSn9cofbI">fundamental features</a> of Pytest
match our needs very well:</p>
<ul class="simple">
<li><p>Automatic test discovery, no boiler-plate test code</p></li>
<li><p>Useful information when a test fails (assertions are introspected)</p></li>
<li><p>Test and fixture <a class="reference external" href="https://docs.pytest.org/en/stable/parametrize.html">parameterization</a></p></li>
<li><p>Modular setup/teardown via <a class="reference external" href="https://docs.pytest.org/en/stable/fixture.html">fixtures</a></p></li>
<li><p>Incredibly customizable (as detailed above)</p></li>
</ul>
<p>All the logic for describing, discovering, running, skipping and
reporting results of the tests, as well as enabling and importing
users’ plugins is already written and maintained by the open source
community. This leaves us to focus on our hard and specific problems:
creating an abstraction to launch the necessary targets, organizing
and publishing our tests, and reporting test results upstream. Using
Pytest also allows us the space to abstract other commonalities in our
specific tests. In this way, LISAv3 could solve the difficulties we
have at hand without creating yet another test framework.</p>
<p>By leveraging such a popular framework we maximize the ease of
adoption for developers to write tests, as they are likely already
familiar with Pytest, and if not, have a wealth of examples and
<a class="reference external" href="https://docs.pytest.org/en/stable/example/index.html">resources</a> from which to draw. The environment will be one of
instant familiarity, thus providing developers a running start.</p>
<p>Finally, by reducing the amount of code we maintain, we drastically
increase our chances of receiving pull requests instead of bug reports
from users. This is important because despite our best efforts it is
practically guaranteed that as adoption of LISAv3 increases, users
will want changes to be made, and we need to empower them to do so
themselves. Using Pytest gives us the best chances for users to
understand and extend the framework, plugins, etc. with ease.</p>
</div>
<div class="section" id="what-are-we-maintaining">
<h2>What are we maintaining?<a class="headerlink" href="#what-are-we-maintaining" title="Permalink to this headline">¶</a></h2>
<p>We have three Pytest plugins, soon to be published on <a class="reference external" href="https://pypi.org/">PyPI</a>, supporting the framework:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#pytest-target">pytest-target</a></p></li>
<li><p><a class="reference internal" href="#pytest-lisa">pytest-lisa</a></p></li>
<li><p><a class="reference internal" href="#pytest-playbook">pytest-playbook</a></p></li>
</ul>
<p>We will also maintain our set of public “LISA” tests, but these are
decoupled from the plugins and packages.</p>
<p>The <a class="reference internal" href="#pytest-target">pytest-target</a> plugin encapsulates all our logic for <em>how</em> and
<em>when</em> to deploy targets (local or cloud virtual machines, or bare
metal machines, and all the associated resources), run tests on the
specified targets, and delete the targets. This includes specifying
which features and resources each test needs and each given target
provides (such as number of cores, amount of RAM, and other hardware
like a GPU etc.), how to deploy and delete each target based on its
platform, and <a class="reference external" href="https://docs.pytest.org/en/stable/parametrize.html">parameterization</a> of the
<a class="reference internal" href="modules/target.plugin.html#target.plugin.target" title="target.plugin.target"><code class="xref py py-func docutils literal notranslate"><span class="pre">target()</span></code></a> fixture based on YAML file input (the
“playbook”). In fact, some tests (like networking) require multiple
targets at once. This plugin will need to manage resources
intelligently, being able to optimize for both time and cost, and make
it easy for tests to request and use various resources.</p>
<p>The <a class="reference internal" href="#pytest-lisa">pytest-lisa</a> plugin encapsulates all our logic for how to
organize and select tests, as well as our opinions on displaying test
results. This includes the user modes, test metadata and inventory,
test selection based on criteria against that metadata, required and
pre-configured upstream plugins, and result notifiers. It will
similarly support YAML file playbook input.</p>
<p>The <a class="reference internal" href="#pytest-playbook">pytest-playbook</a> plugin encapsulates the shared common
functionality of registering component schemata (e.g. platform and
target parameters from <a class="reference internal" href="#pytest-target">pytest-target</a> and selection criteria from
<a class="reference internal" href="#pytest-lisa">pytest-lisa</a>). It uses the <a class="reference external" href="https://pypi.org/project/schema/">schema</a> library.</p>
<p>We have striven to keep <a class="reference internal" href="#pytest-lisa">pytest-lisa</a> and <a class="reference internal" href="#pytest-target">pytest-target</a> from
depending on each other in order to keep their scope well-defined.
They both depend on <a class="reference internal" href="#pytest-playbook">pytest-playbook</a>, and the “LISA” project depends
on them both, but they are independent plugins.</p>
<p>In the “LISA” repository of tests we may also maintain additional
<a class="reference external" href="https://docs.pytest.org/en/stable/fixture.html">fixtures</a> for our tests’ unique requirements. Similarly, we and
others may have private test repositories which build upon the above
by defining new platform support and internal service integrations.
The built-in plugin discovery of Pytest (via <code class="docutils literal notranslate"><span class="pre">conftest.py</span></code> files)
enables us to satisfy one of our requirements to “support plugins to
orchestrate the test environment.”</p>
</div>
<div class="section" id="pytest-target">
<h2>pytest-target<a class="headerlink" href="#pytest-target" title="Permalink to this headline">¶</a></h2>
<div class="section" id="how-are-targets-provided-and-accessed">
<h3>How are targets provided and accessed?<a class="headerlink" href="#how-are-targets-provided-and-accessed" title="Permalink to this headline">¶</a></h3>
<p>First we need to define “target” as an instance of a
system-under-test. That is, given some environment requirements, such
an Azure image (URN) and size (SKU), a target would be a virtual
machine deployed by <a class="reference internal" href="#pytest-target">pytest-target</a> with SSH access provided to the
requesting test. A target could optionally be pre-deployed and simply
connected. Some tests may request multiple targets as well.</p>
<p>Pytest uses <a class="reference external" href="https://docs.pytest.org/en/stable/fixture.html">fixtures</a>, which are the primary way of setting up test
requirements. They replace less flexible alternatives like
setup/teardown functions. It is through fixtures that we implement
remote target setup/teardown. Our <a class="reference internal" href="modules/target.plugin.html#target.plugin.target" title="target.plugin.target"><code class="xref py py-func docutils literal notranslate"><span class="pre">target()</span></code></a>
fixture returns a <a class="reference internal" href="modules/target.target.html#target.target.Target" title="target.target.Target"><code class="xref py py-class docutils literal notranslate"><span class="pre">Target</span></code></a> instance, which
currently provides:</p>
<ul class="simple">
<li><p>Remote shell access via SSH using <a class="reference external" href="https://www.fabfile.org/">Fabric</a></p></li>
<li><p>Data including hostname / IP address</p></li>
<li><p>Cross-platform ping functionality with exponential back-off</p></li>
<li><p>Uploading of local files to arbitrary remote destinations</p></li>
<li><p>Downloading of remote file contents into local string variable</p></li>
<li><p>Asynchronous remote command execution with promises</p></li>
</ul>
<p>The <a class="reference internal" href="modules/target.azure.html#target.azure.AzureCLI" title="target.azure.AzureCLI"><code class="xref py py-class docutils literal notranslate"><span class="pre">AzureCLI</span></code></a> subclass additionally provides:</p>
<ul class="simple">
<li><p>An example of a working platform implementation</p></li>
<li><p>Automatic provisioning of a parameterized Azure VM</p></li>
<li><p>Allowing ICMP ping via Azure firewall rules</p></li>
<li><p>Azure platform forced reboot by API</p></li>
<li><p>Downloading boot diagnostics (serial console log)</p></li>
</ul>
<p>The <a class="reference internal" href="modules/target.target.html#target.target.SSH" title="target.target.SSH"><code class="xref py py-class docutils literal notranslate"><span class="pre">SSH</span></code></a> subclass is a simple implementation
which only connects to a given host.</p>
<p>The <a class="reference internal" href="modules/target.target.html#target.target.Target" title="target.target.Target"><code class="xref py py-class docutils literal notranslate"><span class="pre">Target</span></code></a> class leverages <a class="reference external" href="https://www.fabfile.org/">Fabric</a> which
is a popular high-level Python library for executing shell commands on
remote systems over SSH. Underneath the covers Fabric uses
<a class="reference external" href="https://docs.paramiko.org/en/stable/">Paramiko</a>, the most popular low-level Python SSH library. Fabric
does the heavy lifting of safely connecting and disconnecting from the
node, executing the shell command (synchronously or asynchronously),
reporting the exit status, gathering the <code class="docutils literal notranslate"><span class="pre">stdout</span></code> and <code class="docutils literal notranslate"><span class="pre">stderr</span></code>,
providing <code class="docutils literal notranslate"><span class="pre">stdin</span></code> (or interactive auto-responses, similar to
<code class="docutils literal notranslate"><span class="pre">expect</span></code>), uploading and downloading files, and much more. In fact,
these APIs are all available and implemented for the local machine by
the underlying <a class="reference external" href="https://www.pyinvoke.org/">Invoke</a> library, which is essentially a Python
<code class="docutils literal notranslate"><span class="pre">subprocess</span></code> wrapper with “a powerful and clean feature set.”</p>
<p>Other test specific requirements, such as installing software and
daemons, downloading files from remote storage, or checking the state
of our Bash test scripts, would similarly be implemented by methods on
<a class="reference internal" href="modules/target.target.html#target.target.Target" title="target.target.Target"><code class="xref py py-class docutils literal notranslate"><span class="pre">Target</span></code></a>, its subclasses, or via additional
fixtures and thus shared among tests.</p>
</div>
<div class="section" id="whats-the-target-class">
<h3>What’s the <a class="reference internal" href="modules/target.target.html#target.target.Target" title="target.target.Target"><code class="xref py py-class docutils literal notranslate"><span class="pre">Target</span></code></a> class?<a class="headerlink" href="#whats-the-target-class" title="Permalink to this headline">¶</a></h3>
<p>In version 0.1 of this design document we detailed a planned refactor
of what was then called the <code class="docutils literal notranslate"><span class="pre">Node</span></code> class. This has since been
executed with just a few modifications (one being the rename to
<a class="reference internal" href="modules/target.target.html#target.target.Target" title="target.target.Target"><code class="xref py py-class docutils literal notranslate"><span class="pre">Target</span></code></a>, as <code class="docutils literal notranslate"><span class="pre">Node</span></code> was found to be an
overloaded term in the context of data centers). This class and its
subclasses are decoupled from Pytest, and are used via fixtures. Its
interface looks like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">schema</span> <span class="kn">import</span> <span class="n">Schema</span>
<span class="kn">import</span> <span class="nn">fabric</span>

<span class="k">class</span> <span class="nc">Target</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>

    <span class="n">group</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span>
    <span class="n">features</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
    <span class="n">data</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span>
    <span class="n">number</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">locked</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">host</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">conn</span><span class="p">:</span> <span class="n">fabric</span><span class="o">.</span><span class="n">Connection</span>  <span class="c1"># Provides run, sudo, get, put etc.</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
        <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_schema</span><span class="p">()</span><span class="o">.</span><span class="n">validate</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">number</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">host</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">deploy</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conn</span> <span class="o">=</span> <span class="n">fabric</span><span class="o">.</span><span class="n">Connection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">host</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">schema</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mapping</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Must return a mapping for expected instance parameters.&quot;&quot;&quot;</span>
        <span class="o">...</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">defaults</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mapping</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Can return a mapping for default parameters.&quot;&quot;&quot;</span>
        <span class="o">...</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">deploy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Must deploy the target resources and return the hostname.&quot;&quot;&quot;</span>
        <span class="o">...</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">delete</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Must delete the target&#39;s resources.&quot;&quot;&quot;</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>This class allows us to answer the next question.</p>
<div class="section" id="how-are-new-platforms-supported">
<h4>How are new platforms supported?<a class="headerlink" href="#how-are-new-platforms-supported" title="Permalink to this headline">¶</a></h4>
<p>Platform support is implemented by subclassing
<a class="reference internal" href="modules/target.target.html#target.target.Target" title="target.target.Target"><code class="xref py py-class docutils literal notranslate"><span class="pre">Target</span></code></a> and implementing the abstract
methods in the above interface:</p>
<ul class="simple">
<li><p><a class="reference internal" href="modules/target.target.html#target.target.Target.schema" title="target.target.Target.schema"><code class="xref py py-meth docutils literal notranslate"><span class="pre">schema()</span></code></a>: Define the schema for the platform’s parameters</p></li>
<li><p><a class="reference internal" href="modules/target.target.html#target.target.Target.defaults" title="target.target.Target.defaults"><code class="xref py py-meth docutils literal notranslate"><span class="pre">defaults()</span></code></a>: Define defaults for those parameters</p></li>
<li><p><a class="reference internal" href="modules/target.target.html#target.target.Target.deploy" title="target.target.Target.deploy"><code class="xref py py-meth docutils literal notranslate"><span class="pre">deploy()</span></code></a>: Create an instance resource</p></li>
<li><p><a class="reference internal" href="modules/target.target.html#target.target.Target.delete" title="target.target.Target.delete"><code class="xref py py-meth docutils literal notranslate"><span class="pre">delete()</span></code></a>: Delete the instance and its resources</p></li>
</ul>
<p>Internally we use the <code class="docutils literal notranslate"><span class="pre">__subclasses__</span></code> attribute of
<a class="reference internal" href="modules/target.target.html#target.target.Target" title="target.target.Target"><code class="xref py py-class docutils literal notranslate"><span class="pre">Target</span></code></a> to automatically gather all the
available platforms and their parameter schemata from users’ own
<code class="docutils literal notranslate"><span class="pre">conftest.py</span></code> files and other plugins. This enables the
<a class="reference internal" href="modules/target.plugin.html#target.plugin.target" title="target.plugin.target"><code class="xref py py-func docutils literal notranslate"><span class="pre">target()</span></code></a> fixture to dynamically instantiate a
target from the gathered requirements and parameters.</p>
<p>For example, the <a class="reference internal" href="modules/target.azure.html#target.azure.AzureCLI" title="target.azure.AzureCLI"><code class="xref py py-class docutils literal notranslate"><span class="pre">AzureCLI</span></code></a> subclass defines
its required parameters using the <a class="reference external" href="https://pypi.org/project/schema/">schema</a> library like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">schema</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Schema</span>
<span class="kn">from</span> <span class="nn">target</span> <span class="kn">import</span> <span class="n">Target</span>

<span class="k">class</span> <span class="nc">AzureCLI</span><span class="p">(</span><span class="n">Target</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">schema</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;image&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
            <span class="n">Optional</span><span class="p">(</span><span class="s2">&quot;sku&quot;</span><span class="p">):</span> <span class="nb">str</span><span class="p">,</span>
            <span class="n">Optional</span><span class="p">(</span><span class="s2">&quot;location&quot;</span><span class="p">):</span> <span class="nb">str</span><span class="p">,</span>
        <span class="p">}</span>
</pre></div>
</div>
<p>Simply through defining this subclass the user can now specify a set
of parameterized YAML targets in a playbook like this:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">platforms</span><span class="p">:</span>
  <span class="nt">AzureCLI</span><span class="p">:</span>
    <span class="nt">sku</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Standard_DS2_v2</span>

<span class="nt">targets</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Debian</span>
    <span class="nt">platform</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">AzureCLI</span>
    <span class="nt">image</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Debian:debian-10:10:latest</span>

  <span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Ubuntu</span>
    <span class="nt">platform</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">AzureCLI</span>
    <span class="nt">image</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Canonical:UbuntuServer:18.04-LTS:latest</span>
</pre></div>
</div>
<p>These targets are then used to parameterize the
<a class="reference internal" href="modules/target.plugin.html#target.plugin.target" title="target.plugin.target"><code class="xref py py-func docutils literal notranslate"><span class="pre">target()</span></code></a> fixture in the
<a class="reference internal" href="modules/target.plugin.html#target.plugin.pytest_generate_tests" title="target.plugin.pytest_generate_tests"><code class="xref py py-func docutils literal notranslate"><span class="pre">pytest_generate_tests()</span></code></a> hook (see below for
more details).</p>
<p>This demonstrated how we can have platforms define their own schema
and register that schema automatically. The <code class="docutils literal notranslate"><span class="pre">platforms</span></code> key allows a
playbook to override the defaults in the platform implementation,
which are then eclipsed for each named target in the <code class="docutils literal notranslate"><span class="pre">targets</span></code> key.
This is accomplished through internal details in <a class="reference internal" href="#pytest-target">pytest-target</a>’s
hook implementation <a class="reference internal" href="modules/target.plugin.html#target.plugin.pytest_playbook_schema" title="target.plugin.pytest_playbook_schema"><code class="xref py py-func docutils literal notranslate"><span class="pre">pytest_playbook_schema()</span></code></a>
using the <a class="reference internal" href="#pytest-playbook">pytest-playbook</a> plugin, but for the users, it just works.</p>
</div>
</div>
<div class="section" id="how-do-we-interact-with-azure">
<h3>How do we interact with Azure?<a class="headerlink" href="#how-do-we-interact-with-azure" title="Permalink to this headline">¶</a></h3>
<p>For <a class="reference internal" href="modules/target.azure.html#target.azure.AzureCLI" title="target.azure.AzureCLI"><code class="xref py py-class docutils literal notranslate"><span class="pre">AzureCLI</span></code></a>, we use the <a class="reference external" href="https://aka.ms/azureclidocs">Azure CLI</a> to
deploy a virtual machine. For Hyper-V (and other virtualization
platforms), we would like to use <a class="reference external" href="https://libvirt.org/python.html">libvirt</a>, and for embedded / bare
metal environments we are evaluating <a class="reference external" href="https://github.com/labgrid-project/labgrid">Labgrid</a>.</p>
<p>If possible, we do not want to use the <a class="reference external" href="https://aka.ms/azsdk/python/all">Azure Python APIs</a> directly because they are more
complicated (and less documented) than the <a class="reference external" href="https://aka.ms/azureclidocs">Azure CLI</a>. With
<a class="reference external" href="https://www.pyinvoke.org/">Invoke</a> (as discussed above), <code class="docutils literal notranslate"><span class="pre">az</span></code> becomes incredibly easy to work
with. The Azure CLI lead developer states that they have <a class="reference external" href="https://stackoverflow.com/a/50005660/1028665">feature
parity</a> and that the
CLI is more straightforward to use. Considering our
ease-of-maintenance requirement, this seems the apt choice, especially
since the Azure CLI supports deploying resources with <a class="reference external" href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/deploy-cli">ARM templates</a>.</p>
<p>If it later becomes necessary to use the Python APIs directly, that
is, of course, still doable (and we can reuse existing code doing it).
This implementation can coexist as simply another class, <code class="docutils literal notranslate"><span class="pre">AzureAPI</span></code>.</p>
<p>On the topic of “servicing” the <a class="reference external" href="https://aka.ms/azureclidocs">Azure CLI</a>, its developers state
that “at command level, packages only upgrading the PATCH version
guarantee backward compatibility.” The tool is also intended to be
used in scripts, so servicing would amount to documenting the tested
version and having the Azure class check that it’s compatible before
using it (or warning and then trying its best).</p>
<div class="section" id="how-are-requirements-examined">
<h4>How are requirements examined?<a class="headerlink" href="#how-are-requirements-examined" title="Permalink to this headline">¶</a></h4>
<p>The <a class="reference internal" href="modules/target.target.html#target.target.TargetData.features" title="target.target.TargetData.features"><code class="xref py py-attr docutils literal notranslate"><span class="pre">features</span></code></a> attribute is
currently a list of strings and (combined with the
<a class="reference internal" href="modules/target.target.html#target.target.TargetData.params" title="target.target.TargetData.params"><code class="xref py py-attr docutils literal notranslate"><span class="pre">params</span></code></a> dictionary) is used to
demonstrate how we can test if an existing target instance
(representing a deployed machine) met a test’s requirements. It should
be updated with a <code class="docutils literal notranslate"><span class="pre">Requirements</span></code> class that represents all physical
attributes of the target. The <a class="reference internal" href="modules/target.plugin.html#module-target.plugin" title="target.plugin"><code class="xref py py-mod docutils literal notranslate"><span class="pre">target.plugin</span></code></a> module defines a
<code class="docutils literal notranslate"><span class="pre">&#64;pytest.mark.target</span></code> <a class="reference external" href="https://docs.pytest.org/en/stable/mark.html">pytest-mark</a> which takes the features list
but should instead take instances of this <code class="docutils literal notranslate"><span class="pre">Requirements</span></code> class. Two
<code class="docutils literal notranslate"><span class="pre">Requirements</span></code> should be comparable to determine if one set meets
(or exceeds) the other set. Existing code that does this can be reused
for this.</p>
</div>
<div class="section" id="how-do-we-share-common-tasks">
<h4>How do we share common tasks?<a class="headerlink" href="#how-do-we-share-common-tasks" title="Permalink to this headline">¶</a></h4>
<p>Common tasks for targets like rebooting and pinging should be
implemented on the <a class="reference internal" href="modules/target.target.html#target.target.Target" title="target.target.Target"><code class="xref py py-class docutils literal notranslate"><span class="pre">Target</span></code></a> class, and
platform-specific tasks on the respective subclass.</p>
<p>Methods available from <a class="reference internal" href="modules/target.target.html#target.target.Target.conn" title="target.target.Target.conn"><code class="xref py py-attr docutils literal notranslate"><span class="pre">conn</span></code></a> include
<code class="docutils literal notranslate"><span class="pre">run()</span></code> and <code class="docutils literal notranslate"><span class="pre">sudo()</span></code> which are used to easily run arbitrary
commands, and <code class="docutils literal notranslate"><span class="pre">get()</span></code> and <code class="docutils literal notranslate"><span class="pre">put()</span></code> to download and upload arbitrary
files.</p>
<p>The <a class="reference internal" href="modules/target.target.html#target.target.Target.cat" title="target.target.Target.cat"><code class="xref py py-meth docutils literal notranslate"><span class="pre">cat()</span></code></a> method wraps <code class="docutils literal notranslate"><span class="pre">get()</span></code> and
returns the file as data in a string. This makes test code like this
possible:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">assert</span> <span class="n">target</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="s2">&quot;state.txt&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;TestCompleted&quot;</span>
</pre></div>
</div>
<p>A <code class="docutils literal notranslate"><span class="pre">reboot()</span></code> method should be added that first tries to use
<code class="docutils literal notranslate"><span class="pre">sudo(&quot;reboot&quot;,</span> <span class="pre">timeout=5)</span></code> (with a short timeout to avoid a hung SSH
session). It should retry with an exponential back-off to see if the
machine has rebooted by checking either <code class="docutils literal notranslate"><span class="pre">uptime</span></code> or the existence of a
file created before the reboot. This is to avoid having to <code class="docutils literal notranslate"><span class="pre">sleep()</span></code>
and just guess the amount of time it takes to reboot.</p>
<p>The <code class="xref py py-meth docutils literal notranslate"><span class="pre">restart()</span></code> method should “power
cycle” the machine using the platform’s API, and thus is in abstract
method as each platform needs to implement it differently.</p>
<p>Other tools and shared logic should be implemented as necessary. A major
area of concern is the automatic and package-manager agnostic
installation of necessary tools, much of which has been implemented
previously and can be reused.</p>
</div>
</div>
<div class="section" id="how-are-targets-requested-and-managed">
<h3>How are targets requested and managed?<a class="headerlink" href="#how-are-targets-requested-and-managed" title="Permalink to this headline">¶</a></h3>
<p>In version 0.3 of this design document we detailed how we used a
session-scoped <code class="docutils literal notranslate"><span class="pre">pool()</span></code> fixture to manage targets across an entire
test session. This has since been replaced with an enhanced disk-based
<a class="reference external" href="https://docs.pytest.org/en/stable/cache.html">cache</a>, accessed through a context manager with an atomic file lock:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">filelock</span> <span class="kn">import</span> <span class="n">FileLock</span>

<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">target_pool</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Exclusive access to the cached targets pool.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">lock</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">makedir</span><span class="p">(</span><span class="s2">&quot;target&quot;</span><span class="p">))</span> <span class="o">/</span> <span class="s2">&quot;pool.lock&quot;</span>
    <span class="k">with</span> <span class="n">FileLock</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">lock</span><span class="p">)):</span>
        <span class="n">pool</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;target/pool&quot;</span><span class="p">,</span> <span class="p">{})</span>
        <span class="k">yield</span> <span class="n">pool</span>
        <span class="n">config</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;target/pool&quot;</span><span class="p">,</span> <span class="n">pool</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the cross-session <a class="reference external" href="https://docs.pytest.org/en/stable/cache.html">cache</a> is provided by Pytest, and very
easy to work with. The key maps to a file path, and the data stored
and read is JSON. So our targets are serializable: internally the
<a class="reference external" href="https://docs.python.org/3/library/dataclasses.html">data class</a> <a class="reference internal" href="modules/target.target.html#target.target.TargetData" title="target.target.TargetData"><code class="xref py py-attr docutils literal notranslate"><span class="pre">TargetData</span></code></a> implements the
methods <a class="reference internal" href="modules/target.target.html#target.target.TargetData.to_json" title="target.target.TargetData.to_json"><code class="xref py py-meth docutils literal notranslate"><span class="pre">to_json()</span></code></a> and
<a class="reference internal" href="modules/target.target.html#target.target.TargetData.from_json" title="target.target.TargetData.from_json"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_json()</span></code></a>, and the
<a class="reference internal" href="modules/target.plugin.html#target.plugin.target" title="target.plugin.target"><code class="xref py py-func docutils literal notranslate"><span class="pre">target()</span></code></a> fixture creates new instances of
<a class="reference internal" href="modules/target.target.html#target.target.Target" title="target.target.Target"><code class="xref py py-class docutils literal notranslate"><span class="pre">Target</span></code></a> for the requesting test from either
a “fit” cached target (and so locks it) or deploys a new target.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span> <span class="nf">target</span><span class="p">(</span><span class="n">request</span><span class="p">:</span> <span class="n">SubRequest</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Target</span><span class="p">]:</span>
    <span class="o">...</span>
    <span class="k">with</span> <span class="n">target_pool</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">config</span><span class="p">)</span> <span class="k">as</span> <span class="n">pool</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">json</span> <span class="ow">in</span> <span class="n">pool</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">fits</span><span class="p">(</span><span class="n">TargetData</span><span class="p">(</span><span class="o">**</span><span class="n">json</span><span class="p">)):</span>
                <span class="n">t</span> <span class="o">=</span> <span class="n">Target</span><span class="o">.</span><span class="n">from_json</span><span class="p">(</span><span class="n">json</span><span class="p">)</span>
                <span class="n">t</span><span class="o">.</span><span class="n">locked</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="n">pool</span><span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">to_json</span><span class="p">()</span>
        <span class="c1"># Or...</span>
        <span class="bp">cls</span> <span class="o">=</span> <span class="n">Target</span><span class="o">.</span><span class="n">get_platform</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;platform&quot;</span><span class="p">])</span>
        <span class="n">t</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="p">{},</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">pool</span><span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">to_json</span><span class="p">()</span>
</pre></div>
</div>
<p>Because all access to the cache (and so the target pool) is within the
scope of the context manager, the access is locked in such a way that
this works with multiple Pytest processes, as used by <a class="reference external" href="https://github.com/pytest-dev/pytest-xdist">pytest-xdist</a>
and as necessary for parallel CPU-bound tasks (like testing multiple
targets) given Python’s <a class="reference external" href="https://wiki.python.org/moin/GlobalInterpreterLock">Global Interpreter Lock</a>. Platform
implementations can save arbitrary JSON-serializable data to the
class’s <a class="reference internal" href="modules/target.target.html#target.target.TargetData.data" title="target.target.TargetData.data"><code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code></a> attribute and it
will be returned when recreated from the cache.</p>
<p>While currently an unordered dictionary, to support optimal scheduling
we will likely want to use a priority queue, where the priority of a
target represents its cost (whether in terms of time or money),
allowing us to provide either the fastest or the cheapest target to
each request. By using the <a class="reference external" href="https://docs.pytest.org/en/latest/reference.html#pytest.hookspec.pytest_collection_modifyitems">pytest_collection_modifyitems</a> hook to
sort (and so group) the tests by their requirements, the tests would
efficiently reuse targets. Except for the most recently used target,
targets not in use (unlocked) should be deallocated.</p>
<p>With the <code class="docutils literal notranslate"><span class="pre">--keep-targets</span></code> CLI flag the targets won’t be deleted at
the end of a run, and without it they will be automatically deleted.
Regardless, they will always be cached to disk when they are created
so that the CLI flag <code class="docutils literal notranslate"><span class="pre">--delete-targets</span></code> can delete <em>all</em> allocated
targets, even after a test session is interrupted.</p>
<p>The fixture is indirectly parameterized during setup with the
<a class="reference internal" href="modules/target.plugin.html#target.plugin.pytest_generate_tests" title="target.plugin.pytest_generate_tests"><code class="xref py py-func docutils literal notranslate"><span class="pre">pytest_generate_tests()</span></code></a> hook. Test and fixture
<a class="reference external" href="https://docs.pytest.org/en/stable/parametrize.html">parameterization</a> is a huge feature of Pytest. When we parameterize
the <a class="reference internal" href="modules/target.plugin.html#target.plugin.target" title="target.plugin.target"><code class="xref py py-func docutils literal notranslate"><span class="pre">target()</span></code></a> fixture for multiple targets
(e.g. “Ubuntu” and “Debian”), Pytest automatically creates a set of
tests for each target. So <code class="docutils literal notranslate"><span class="pre">test_smoke</span></code> turns into
<code class="docutils literal notranslate"><span class="pre">test_smoke[Ubuntu]</span></code> and <code class="docutils literal notranslate"><span class="pre">test_smoke[Debian]</span></code>. This allows us to
run a collection of tests against multiple targets with ease. These
targets are defined in a YAML file (thanks to <a class="reference internal" href="#pytest-playbook">pytest-playbook</a>) and
validated against the parameters collected from the previously
described platform subclasses.</p>
<p>Finally, once the <a class="reference internal" href="modules/target.plugin.html#target.plugin.target" title="target.plugin.target"><code class="xref py py-func docutils literal notranslate"><span class="pre">target()</span></code></a> fixture has
returned a working and sanity-checked environment to the requesting
test, the test is capable of examining any and all attributes of the
<a class="reference internal" href="modules/target.target.html#target.target.Target" title="target.target.Target"><code class="xref py py-class docutils literal notranslate"><span class="pre">Target</span></code></a> and quickly marking itself as
skipped, expected to fail, or failed before executing the body of the
test. Our static type checking enables developers to ensure that the
platform they requested supports all methods and fields they use by
annotating the test’s <code class="docutils literal notranslate"><span class="pre">target</span></code> parameter with the expected platform
type (or types). Ensuring the effectiveness of this type checking will
require us to carefully update our platform implementations, and not
rely on arbitrary objects of data. (For example, add an
<code class="docutils literal notranslate"><span class="pre">internal_address</span></code> field to <code class="docutils literal notranslate"><span class="pre">AzureCLI</span></code>, don’t just look up
<code class="docutils literal notranslate"><span class="pre">data[&quot;internal_address&quot;]</span></code>.)</p>
</div>
</div>
<div class="section" id="pytest-lisa">
<h2>pytest-lisa<a class="headerlink" href="#pytest-lisa" title="Permalink to this headline">¶</a></h2>
<div class="section" id="how-are-tests-described">
<h3>How are tests described?<a class="headerlink" href="#how-are-tests-described" title="Permalink to this headline">¶</a></h3>
<p>The built-in <a class="reference external" href="https://docs.pytest.org/en/stable/mark.html">pytest-mark</a> plugin already provides functionality for
adding metadata to tests, where we specifically want (and describe
using <a class="reference external" href="https://pypi.org/project/schema/">schema</a> <a class="reference internal" href="modules/lisa.html#lisa.lisa_schema" title="lisa.lisa_schema"><code class="xref py py-data docutils literal notranslate"><span class="pre">lisa_schema</span></code></a>):</p>
<ul class="simple">
<li><p>Platform: used to skip tests inapplicable to the current
system-under-test</p></li>
<li><p>Category: our high-level test organization</p></li>
<li><p>Area: feature being tested</p></li>
<li><p>Priority: self-explanatory</p></li>
<li><p>Tags: optional additional metadata for test organization</p></li>
<li><p>Features: a set of required features (like “GPU”)</p></li>
<li><p>Reuse: a boolean to indicate if a target is reusable after the test</p></li>
<li><p>Count: number of targets the test needs</p></li>
</ul>
<p>We simply reuse this with minimal logic to enforce our required
metadata, with sane defaults , and to list statistics about our test
coverage. It looks like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lisa</span> <span class="kn">import</span> <span class="n">LISA</span>

<span class="nd">@LISA</span><span class="p">(</span><span class="n">platform</span><span class="o">=</span><span class="s2">&quot;Azure&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="s2">&quot;Functional&quot;</span><span class="p">,</span> <span class="n">area</span><span class="o">=</span><span class="s2">&quot;deploy&quot;</span><span class="p">,</span> <span class="n">priority</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_smoke</span><span class="p">(</span><span class="n">target</span><span class="p">:</span> <span class="n">AzureCLI</span><span class="p">,</span> <span class="n">caplog</span><span class="p">:</span> <span class="n">LogCaptureFixture</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="s2">&quot;&quot;&quot;Check that an Azure Linux VM can be deployed and is responsive.</span>
</pre></div>
</div>
<p>This is a functional example. With this simple decorator, all test
<a class="reference external" href="https://docs.pytest.org/en/latest/reference.html#collection-hooks">collection hooks</a> can introspect the metadata, enforce required
parameters and set defaults, select tests based on arbitrary criteria,
and list test coverage statistics (test inventory). We validate the
metadata in <a class="reference internal" href="modules/lisa.html#lisa.pytest_collection_modifyitems" title="lisa.pytest_collection_modifyitems"><code class="xref py py-func docutils literal notranslate"><span class="pre">lisa.pytest_collection_modifyitems()</span></code></a>.</p>
<p>Note that Pytest leverages Python’s docstrings for built-in
documentation (and can even run tests discovered in such strings, like
doctest). Hence we do not have a separate field for the test’s
documentation. By following the best practice of using docstrings for
our modules, classes, and functions, we can automatically to generate
full <a class="reference external" href="https://microsoft.github.io/lisa/.">documentation</a> for each plugin and test (which you are likely
currently reading).</p>
<p>This mark also does need to be repeated for each test, as marks can be
scoped to a module, and so one line could describe defaults for every
test in a file, with individual tests overriding parameters as needed.</p>
<p>In the current implementation, we take a <code class="docutils literal notranslate"><span class="pre">features:</span> <span class="pre">List[str]</span></code>
argument that is used to prove the concept deploying (or reusing) a
target based on the test’s required and the target’s available sets of
features, and it is passed to <code class="docutils literal notranslate"><span class="pre">&#64;pytest.mark.target</span></code>. See <a class="reference internal" href="#how-are-requirements-examined">How are
requirements examined?</a> for more. Coupled with the test’s requested
<a class="reference internal" href="modules/target.plugin.html#target.plugin.target" title="target.plugin.target"><code class="xref py py-func docutils literal notranslate"><span class="pre">target()</span></code></a> fixture being parameterized (see
discussion in <a class="reference internal" href="#pytest-target">pytest-target</a>) this demonstrates at least one way we
can satisfy our “test run planner/scheduler” requirement.</p>
<p>Furthermore, we have a prototype <a class="reference external" href="https://github.com/LIS/LISAv2/tree/pytest/generator">generator</a> which parses
LISAv2 XML test descriptions and generates stubs with this mark filled
in correctly.</p>
</div>
<div class="section" id="how-are-tests-selected">
<h3>How are tests selected?<a class="headerlink" href="#how-are-tests-selected" title="Permalink to this headline">¶</a></h3>
<p>Pytest already allows a user to specify which exact tests to run:</p>
<ul class="simple">
<li><p>Listing folders on the CLI (see below on where tests should live)</p></li>
<li><p>Specifying a name expression on the CLI (e.g. <code class="docutils literal notranslate"><span class="pre">-k</span> <span class="pre">smoke</span> <span class="pre">and</span> <span class="pre">xdp</span></code>)</p></li>
<li><p>Specifying a mark expression on the CLI (e.g. <code class="docutils literal notranslate"><span class="pre">-m</span> <span class="pre">functional</span> <span class="pre">and</span>
<span class="pre">not</span> <span class="pre">slow</span></code>)</p></li>
</ul>
<p>We can also implement any other mechanism via the
<a class="reference external" href="https://docs.pytest.org/en/latest/reference.html#pytest.hookspec.pytest_collection_modifyitems">pytest_collection_modifyitems</a> hook. The existing implementation in <a class="reference internal" href="modules/lisa.html#module-lisa" title="lisa"><code class="xref py py-mod docutils literal notranslate"><span class="pre">lisa</span></code></a>
supports gathering selection criteria from a YAML file:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">criteria</span><span class="p">:</span>
  <span class="c1"># Select all Priority 0 tests.</span>
  <span class="p p-Indicator">-</span> <span class="nt">priority</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
  <span class="c1"># Run tests with &#39;smoke&#39; in the name twice.</span>
  <span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">smoke</span>
    <span class="nt">times</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2</span>
  <span class="c1"># Exclude all tests in Area &quot;xdp&quot;</span>
  <span class="p p-Indicator">-</span> <span class="nt">area</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">xdp</span>
    <span class="nt">exclude</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
<p>This criteria is validated against following <a class="reference external" href="https://pypi.org/project/schema/">schema</a> defined in
<a class="reference internal" href="modules/lisa.html#lisa.pytest_playbook_schema" title="lisa.pytest_playbook_schema"><code class="xref py py-func docutils literal notranslate"><span class="pre">lisa.pytest_playbook_schema()</span></code></a>.</p>
<p>The test collection is then modified using the Pytest hook in
<a class="reference internal" href="modules/lisa.html#lisa.pytest_collection_modifyitems" title="lisa.pytest_collection_modifyitems"><code class="xref py py-func docutils literal notranslate"><span class="pre">lisa.pytest_collection_modifyitems()</span></code></a>. Because this is simply
a Python list, we can also sort the tests according to our needs, such
as by priority. If the <a class="reference internal" href="#pytest-target">pytest-target</a> plugin has already sorted by
requirements, that’s just fine, Python’s <code class="docutils literal notranslate"><span class="pre">sorted()</span></code> built-in is
guaranteed to be stable (meaning we can sort in multiple passes).</p>
<p>Together, the CLI support and YAML playbook satisfy one of our “test
entrance” requirements. We also generate our own binary called
<code class="docutils literal notranslate"><span class="pre">lisa</span></code> which simply delegates to Pytest.</p>
</div>
<div class="section" id="how-are-results-reported">
<h3>How are results reported?<a class="headerlink" href="#how-are-results-reported" title="Permalink to this headline">¶</a></h3>
<p>Parsing the results of a large test suite can be difficult.
Fortunately, because Pytest is a testing framework, there already
exists support for generating excellent reports. For developers, the
<a class="reference external" href="https://pypi.org/project/pytest-html/">HTML report</a> is easy to read: it is self-contained, holds all the
results and logs, and each test can be expanded and collapsed. Tests
which were rerun are recorded separately. For CI pipelines, Pytest has
integrated <a class="reference external" href="https://docs.pytest.org/en/stable/_modules/_pytest/junitxml.html">JUnit</a>
XML test report support. This is the standard method of reporting
results to CI servers like Jenkins and are natively parsed into the CI
system’s built-in test display page. Finally, Azure DevOps pipelines
are even supported with a community plugin <a class="reference external" href="https://pypi.org/project/pytest-azurepipelines/">pytest-azurepipelines</a> which enhances the
standard JUnit report for ADO.</p>
<p>One of our requirements is to support the lookup of previous tests’
execution metrics, such as recorded performance metrics and duration,
so that performance tests can check regressions. This is the perfect
example of carrying a small fixture which provides access to our
internal database and is dynamically added to our tests when run
internally, and the tests can lookup and record whatever they need
through the fixture.</p>
<p>However, we also have internal requirements to report test results
throughout the test life cycle to a database (the “result manager” and
“progress tracker”) to be consumed by other tools. In this sense,
LISAv3 (the composition of our published plugins, tests, and fixtures)
is simply a producer, and the consumers can parse the test results,
send emails, archive the collected logs, update a GUI display of test
progress, etc. Our repository’s <code class="docutils literal notranslate"><span class="pre">conftest.py</span></code> can implement the
necessary logic using Pytest’s ample <a class="reference external" href="https://docs.pytest.org/en/latest/reference.html#test-running-runtest-hooks">test running hooks</a>.
In particular, the hook <a class="reference external" href="https://docs.pytest.org/en/latest/reference.html#pytest.hookspec.pytest_runtest_makereport">pytest_runtest_makereport</a>
is called for each of the setup, call and teardown phases of a test.
As such it can used for precisely this purpose.</p>
</div>
<div class="section" id="how-is-setup-run-and-cleanup-handled">
<h3>How is setup, run, and cleanup handled?<a class="headerlink" href="#how-is-setup-run-and-cleanup-handled" title="Permalink to this headline">¶</a></h3>
<p>Pytest strives to require minimal boiler-plate code. Thus the classic
“xunit-style” of defining a class with setup and teardown functions in
addition to test functions is not recommended (nor necessary).
Generally Pytest expects <a class="reference external" href="https://docs.pytest.org/en/stable/fixture.html">fixtures</a> to be used for dependency
injection (which is what setup/teardown functions usually do). For
users that really want the classic style, it is nonetheless fully
<a class="reference external" href="https://docs.pytest.org/en/stable/xunit_setup.html">supported</a> and
documented (and can be applied at the module, class, and method
scopes). Thus our “test runner” requirement is satisfied.</p>
</div>
<div class="section" id="how-are-tests-timed-out">
<h3>How are tests timed out?<a class="headerlink" href="#how-are-tests-timed-out" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference external" href="https://pypi.org/project/pytest-timeout/">pytest-timeout</a>
plugin provides integrated timeouts via <code class="docutils literal notranslate"><span class="pre">&#64;pytest.mark.timeout(&lt;N</span>
<span class="pre">seconds&gt;)</span></code>, a configuration file option, environment variable, and
CLI flag. The Fabric library provides timeouts in both the
configuration and per-command usage. These are already used to
satisfaction in the prototype. Additionally, Pytest has built-in
support for measuring the duration of each fixture’s setup and
teardown and each test (it’s simply the <code class="docutils literal notranslate"><span class="pre">--durations</span></code> and
<code class="docutils literal notranslate"><span class="pre">--durations-min</span></code> flags).</p>
</div>
<div class="section" id="how-are-tests-organized">
<h3>How are tests organized?<a class="headerlink" href="#how-are-tests-organized" title="Permalink to this headline">¶</a></h3>
<p>That is, what does a folder of tests map to: a platform, feature, or
owner?</p>
<p>In the author’s opinion it is likely to be both. Tests which are
common to a platform and written by our team are probably best placed
in a folder like <code class="docutils literal notranslate"><span class="pre">tests/azure</span></code> whereas tests for a particular
scenario which limits their image and SKU applicability should be in a
folder like <code class="docutils literal notranslate"><span class="pre">tests/acc</span></code>. It’s going to depend on how often the tests
are run together.</p>
<p>Because Pytest can run tests and <code class="docutils literal notranslate"><span class="pre">conftest.py</span></code> files from arbitrary
folders, maintaining sets of tests and plugins separately from the
base LISA repository is easy. Custom repositories with new tests,
plugins, fixtures, platform-specific support, etc. can simply be
cloned anywhere, and provided on the command-line to Pytest.</p>
<p>Test authors should keep tests which share requirements and are
otherwise similar to a single module (Python file). Not only is this
well-organized, but because marks can be applied at the module level,
setting all the tests to be skipped or expected to fail (with the
built-in <code class="docutils literal notranslate"><span class="pre">skip</span></code> and <code class="docutils literal notranslate"><span class="pre">xfail</span></code> Pytest marks) becomes even easier.</p>
<p>An open question is if we really want to bring every test from LISAv2
directly over, or if we should carefully analyze our tests to craft a
new set of high-level scenarios. An interesting result of reorganizing
and rewriting the tests would be the ability to have test layers,
where the result of a high-level test dictates if the tests below it
should be skipped. If it passes, it implies the tests underneath it
would pass, and so skips them; but if it fails, the next test below it
runs and so on until a passing layer is found.</p>
</div>
<div class="section" id="how-will-we-port-lisav2-tests">
<h3>How will we port LISAv2 tests?<a class="headerlink" href="#how-will-we-port-lisav2-tests" title="Permalink to this headline">¶</a></h3>
<p>Given the above, we still must decide if we want to put the
engineering effort into porting <em>every</em> LISAv2 test. However, the
prototype started by porting the <code class="docutils literal notranslate"><span class="pre">LIS-DRIVER-VERSION-CHECK</span></code> test,
proving that tests which exclusively use Bash scripts are trivially
portable. Unfortunately, most tests use an associated PowerShell
script which is tightly coupled to the LISAv2 framework.</p>
<p>We believe that it is <em>possible</em> to port these tests without untoward
modifications. We would need to write a mock library that implements
(or stubs where appropriate) LISAv2 framework functionality such as
<code class="docutils literal notranslate"><span class="pre">Provision-VMsForLisa</span></code>, <code class="docutils literal notranslate"><span class="pre">Copy-RemoteFiles</span></code>, <code class="docutils literal notranslate"><span class="pre">Run-LinuxCmd</span></code>,
etc., and provides both the expected “global” objects and the test
function parameters <code class="docutils literal notranslate"><span class="pre">AllVmData</span></code> and <code class="docutils literal notranslate"><span class="pre">CurrentTestData</span></code>. But it
wouldn’t be great.</p>
<p>This work needs to be done regardless of the approach we take with our
framework (leveraging Pytest or writing our own), and it is not
inconsequential work. It needs to be thoroughly planned and executed,
and is certainly a ways off. The author’s personal opinion is that we
won’t want to port most LISAv2 tests, and instead create a new set of
well-documented, comprehensive, layered tests that cover our current
needs, instead of bringing along all these historical tests.</p>
</div>
<div class="section" id="how-are-tests-and-functions-retried">
<h3>How are tests and functions retried?<a class="headerlink" href="#how-are-tests-and-functions-retried" title="Permalink to this headline">¶</a></h3>
<p>Testing remote targets is inherently flaky, so we take a two-pronged
approach to dealing with the flakiness.</p>
<p>The <a class="reference external" href="https://pypi.org/project/pytest-rerunfailures/">pytest-rerunfailures</a> plugin can be used to easily mark a test
itself as flaky. It has the nice feature of recording each rerun in
the produced report. It looks like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">flaky</span><span class="p">(</span><span class="n">reruns</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_something_flaky</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This fails most of the time.&quot;&quot;&quot;</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Note that there is an open <a class="reference external" href="https://github.com/pytest-dev/pytest-rerunfailures/issues/51">bug</a> in
this plugin which can cause issues with fixtures using scopes other
than “function” but it can be worked around (and we mostly use
“function” scope anyway).</p>
<p>The <a class="reference external" href="https://tenacity.readthedocs.io/en/latest/">Tenacity</a> library is used to retry flaky functions that are not
tests, such as downloading boot diagnostics or pinging a node. As the
“modern Python retry library” it has easy-to-use decorators to retry
functions (and context managers to use within functions), as well as
excellent wait and timeout support. The
<a class="reference internal" href="modules/target.target.html#target.target.Target.ping" title="target.target.Target.ping"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ping()</span></code></a> method looks like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tenacity</span> <span class="kn">import</span> <span class="n">retry</span><span class="p">,</span> <span class="n">stop_after_attempt</span><span class="p">,</span> <span class="n">wait_exponential</span>

<span class="k">class</span> <span class="nc">Target</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="nd">@retry</span><span class="p">(</span><span class="n">reraise</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">wait</span><span class="o">=</span><span class="n">wait_exponential</span><span class="p">(),</span> <span class="n">stop</span><span class="o">=</span><span class="n">stop_after_attempt</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">ping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Result</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Ping the node from the local system in a cross-platform manner.&quot;&quot;&quot;</span>
        <span class="n">flag</span> <span class="o">=</span> <span class="s2">&quot;-c 1&quot;</span> <span class="k">if</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;Linux&quot;</span> <span class="k">else</span> <span class="s2">&quot;-n 1&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">local</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ping </span><span class="si">{</span><span class="n">flag</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">host</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>We can additionally list a test twice when modifying the items
collection, as implemented in the criteria proof-of-concept. However,
given the above abilities, this may not be desired.</p>
</div>
<div class="section" id="how-are-tests-executed-in-parallel">
<h3>How are tests executed in parallel?<a class="headerlink" href="#how-are-tests-executed-in-parallel" title="Permalink to this headline">¶</a></h3>
<p>While our original list of goals stated that we want to run tests “in
parallel” we were not specific about what was meant, and the topic of
parallelism and concurrency is understandably complex. We certainly
don’t mean running two tests at once on the same target, as this would
undoubtedly lead to flaky tests.</p>
<p>Assuming that we care about a set of tests passing on a particular
image and size combination, but not necessarily on a particular
deployed instance, then we can run tests concurrently by deploying
multiple “identical” targets and splitting the tests across them. The
tests would still run in isolation on each target. This sounds hard,
but actually it’s practically free with Pytest via <a class="reference external" href="https://github.com/pytest-dev/pytest-xdist">pytest-xdist</a>.</p>
<p>The default <a class="reference external" href="https://github.com/pytest-dev/pytest-xdist">pytest-xdist</a> implementation simply takes the list of
tests and runs them in a round-robin fashion with the desired number
of executors. We’ve talked at length about being able to schedule
groups of tests to run in particular executors and using particular
targets. While there are many paths open to us, this plugin actually
provides a hook, <a class="reference external" href="https://github.com/pytest-dev/pytest-xdist/blob/master/OVERVIEW.md">pytest_xdist_make_scheduler</a>
that exists specifically to “implement custom tests distribution
logic.” We used this to create the <a class="reference internal" href="modules/lisa.html#lisa.LISAScheduling" title="lisa.LISAScheduling"><code class="xref py py-class docutils literal notranslate"><span class="pre">LISAScheduling</span></code></a>
custom scheduler.</p>
<p>Figuring out the requirements of our test scheduler and designing the
best algorithm will require further discussion and design review. For
the purposes of moving forward, we are not blocked, as the eventual
implementation can be dropped in-place with minimal effort.</p>
</div>
<div class="section" id="what-are-the-user-modes">
<h3>What are the user modes?<a class="headerlink" href="#what-are-the-user-modes" title="Permalink to this headline">¶</a></h3>
<p>Because Pytest is incredibly <a class="reference external" href="https://docs.pytest.org/en/stable/customize.html">customizable</a>, we may want to provide a
few sets of reasonable default configurations for some common
scenarios. We should add a flag like
<code class="docutils literal notranslate"><span class="pre">--lisa-mode=[dev,debug,ci,demo]</span></code> to change the default options and
output of Pytest. Doing so is readily supported by Pytest via the
<a class="reference external" href="https://docs.pytest.org/en/latest/reference.html#pytest.hookspec.pytest_addoption">pytest_addoption</a> and <a class="reference external" href="https://docs.pytest.org/en/latest/reference.html#pytest.hookspec.pytest_configure">pytest_configure</a> hooks. We call these the
provided “user modes.” Note that by “output” we mean not just logging
(because that implies the Python <code class="docutils literal notranslate"><span class="pre">logger</span></code> module, which Pytest
allows full control over) but also commands’ <code class="docutils literal notranslate"><span class="pre">stdout</span></code> and <code class="docutils literal notranslate"><span class="pre">stderr</span></code>
as well as Pytest-provided information.</p>
<p>As the current implementation stands, we just have sane defaults in
our repository’s <code class="docutils literal notranslate"><span class="pre">pytest.ini</span></code>, and users who install and use our
plugins or tests can edit their own <code class="docutils literal notranslate"><span class="pre">pytests.ini</span></code></p>
<ul class="simple">
<li><p>The dev(eloper) mode is intended for use by test developers while
writing a new test. It is verbose, caches the deployed VMs between
runs, and generates a digestible <a class="reference external" href="https://pypi.org/project/pytest-html/">HTML report</a> report.</p></li>
<li><p>The debug mode is like dev mode but with all possible information
shown, and will open the Python debugger automatically on failures
(which is provided by Pytest with the <code class="docutils literal notranslate"><span class="pre">--pdb</span></code> flag).</p></li>
<li><p>The CI mode will be fairly quiet on the console, showing all test
results, but putting the full info output into the generated report
file (HTML for sharing with humans and <a class="reference external" href="https://docs.pytest.org/en/stable/_modules/_pytest/junitxml.html">JUnit</a>
for the associated CI environment, which presents as native test
results).</p></li>
<li><p>The demo mode will show the “executive summary” (a lot like CI, but
finely tuned for demos).</p></li>
</ul>
</div>
</div>
<div class="section" id="pytest-playbook">
<h2>pytest-playbook<a class="headerlink" href="#pytest-playbook" title="Permalink to this headline">¶</a></h2>
<p>This plugin is simple, but exciting. The module <a class="reference internal" href="modules/playbook.html#module-playbook" title="playbook"><code class="xref py py-mod docutils literal notranslate"><span class="pre">playbook</span></code></a>
defines a hook <a class="reference internal" href="modules/playbook.html#playbook.Hooks.pytest_playbook_schema" title="playbook.Hooks.pytest_playbook_schema"><code class="xref py py-meth docutils literal notranslate"><span class="pre">playbook.Hooks.pytest_playbook_schema()</span></code></a> which
other plugins (as discussed above) can use to add schemata to the
final playbook. In <a class="reference internal" href="modules/playbook.html#playbook.pytest_configure" title="playbook.pytest_configure"><code class="xref py py-meth docutils literal notranslate"><span class="pre">playbook.pytest_configure()</span></code></a>, all the
schemata are gathered and then the file given by <code class="docutils literal notranslate"><span class="pre">--playbook=&lt;FILE&gt;</span></code>
is read, validated, and made available at <a class="reference internal" href="modules/playbook.html#playbook.data" title="playbook.data"><code class="xref py py-data docutils literal notranslate"><span class="pre">playbook.data</span></code></a>. It
uses the <a class="reference external" href="https://pyyaml.org/wiki/PyYAMLDocumentation">PyYAML</a>
library, but can be extended to support other formats. Also “YAML
Schema” section in <a class="reference internal" href="CONTRIBUTING.html"><span class="doc">contributing guidelines</span></a> on
how to generate the <a class="reference external" href="https://json-schema.org/">JSON Schema</a> for use
with editors or for manual review.</p>
<p>This is leveraging Pytest’s existing parameterization technology to
achieve one of our “test entrance” goals of requesting environments
with a YAML playbook, and one of our “test parameter validation” goals
of validating platforms before executing tests so that we can fail
fast if a target has insufficient information to be setup. Parsing the
same parameters from a CLI can also be implemented.</p>
</div>
<div class="section" id="what-does-the-flow-of-pytest-look-like">
<h2>What does the “flow” of Pytest look like?<a class="headerlink" href="#what-does-the-flow-of-pytest-look-like" title="Permalink to this headline">¶</a></h2>
<p>This is best described in Pythonic pseudo-code, where the context
manager encapsulates each scope and the for loop encapsulates
processing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pool_fixture</span><span class="p">:</span> <span class="n">a</span> <span class="n">session</span><span class="o">-</span><span class="n">scoped</span> <span class="n">context</span> <span class="n">manager</span>
<span class="n">target_fixture</span><span class="p">:</span> <span class="n">a</span> <span class="n">function</span><span class="o">-</span><span class="n">scoped</span> <span class="n">context</span> <span class="n">manager</span>
<span class="n">items</span><span class="p">:</span> <span class="n">a</span> <span class="n">collection</span> <span class="n">of</span> <span class="n">tests</span>
<span class="n">targets</span><span class="p">:</span> <span class="n">a</span> <span class="n">collection</span> <span class="n">of</span> <span class="n">targets</span>
<span class="n">criteria</span><span class="p">:</span> <span class="n">a</span> <span class="n">collection</span> <span class="n">of</span> <span class="n">test</span> <span class="n">selection</span> <span class="n">criteria</span>

<span class="k">def</span> <span class="nf">pytest_addoption</span><span class="p">(</span><span class="n">parser</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add CLI options etc.&quot;&quot;&quot;</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addoption</span><span class="p">(</span><span class="s2">&quot;--playbook&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">Path</span><span class="p">)</span>

<span class="n">pytest_addoption</span><span class="p">(</span><span class="n">parser</span><span class="p">)</span> <span class="c1"># Pytest fills in parser.</span>

<span class="k">def</span> <span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Setup the run&#39;s configuration.&quot;&quot;&quot;</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">playbook</span><span class="o">.</span><span class="n">get_targets</span><span class="p">()</span>
    <span class="n">criteria</span> <span class="o">=</span> <span class="n">playbook</span><span class="o">.</span><span class="n">get_criteria</span><span class="p">()</span>

<span class="n">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="c1"># Pytest fills in config.</span>

<span class="c1"># pytest_generate_tests(metafunc) does this:</span>
<span class="k">for</span> <span class="n">test_metafunc</span> <span class="ow">in</span> <span class="n">metafuncs</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">targets</span><span class="p">:</span>
        <span class="c1"># items is tests * targets in size</span>
        <span class="n">items</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_metafunc</span><span class="p">[</span><span class="n">target</span><span class="p">])</span>

<span class="c1"># pytest_collection_modifyitems(session, config, items) does this:</span>
<span class="k">for</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">items</span><span class="p">:</span>
    <span class="n">validate</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
    <span class="n">include_or_exclude</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">criteria</span><span class="p">)</span>

<span class="c1"># finally, each executor/session does this:</span>
<span class="n">session_items</span> <span class="o">=</span> <span class="n">items</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="c1"># based on scheduler algorithm</span>
<span class="k">with</span> <span class="n">pool_fixture</span> <span class="k">as</span> <span class="n">pool</span><span class="p">:</span>
    <span class="c1"># the fixture has setup a pool to track the deployed targets</span>
    <span class="k">for</span> <span class="n">test_function</span> <span class="ow">in</span> <span class="n">session_items</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">target_fixture</span> <span class="k">as</span> <span class="n">target</span><span class="p">:</span>
            <span class="c1"># the fixture has found or deployed an appropriate target</span>
            <span class="n">test_function</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="what-else">
<h2>What Else?<a class="headerlink" href="#what-else" title="Permalink to this headline">¶</a></h2>
<p>There’s still a lot more to think about and design. A non-exhaustive
list of future topics (some touched on above):</p>
<ul class="simple">
<li><p>Terminology table</p></li>
<li><p>Tests inventory (generating statistics from metadata)</p></li>
<li><p>Environment / multiple targets class design</p></li>
<li><p>Feature/requirement requests (NICs in particular)</p></li>
<li><p>Custom test scheduler algorithm</p></li>
<li><p>Secret management</p></li>
</ul>
</div>
<div class="section" id="what-alternatives-were-tried">
<h2>What alternatives were tried?<a class="headerlink" href="#what-alternatives-were-tried" title="Permalink to this headline">¶</a></h2>
<p>These are notes from things tried that did not work out, and why.</p>
<div class="section" id="writing-another-framework">
<h3>Writing Another Framework<a class="headerlink" href="#writing-another-framework" title="Permalink to this headline">¶</a></h3>
<p>The author believes the above set of technical specifications clearly
describes how we can leverage Pytest for our needs. Furthermore, the
existing implementation proves this is a viable option. Therefore he
does not think we should write and maintain a <em>new</em> Python testing
framework. We should avoid falling for “not invented here” syndrome.
The alternative prototype which implements a whole new testing
framework required over five thousand lines of code, and the
Pytest-based prototype used less than two hundred (now barely six
hundred as a full fledged implementation with three separate Pytest
plugins, even after extensive feature additions and refactors), or
less than three percent.</p>
<p>We do not want to take on the maintenance cost of yet another
framework, the maintenance cost of LISAv2 already caused this mess in
the first place. The work of prototyping said new framework was
valuable, as it provided insight into the eventual technical design of
LISAv3, as laid out in this document.</p>
</div>
<div class="section" id="using-remote-capabilities-of-pytest-xdist">
<h3>Using Remote Capabilities of <code class="docutils literal notranslate"><span class="pre">pytest-xdist</span></code><a class="headerlink" href="#using-remote-capabilities-of-pytest-xdist" title="Permalink to this headline">¶</a></h3>
<p>With the <a class="reference external" href="https://github.com/pytest-dev/pytest-xdist">pytest-xdist</a>
plugin there already exists support for running a folder of tests on an
arbitrary remote host via SSH.</p>
<p>The LISA tests could be written as Python code suitable for running on
the target test system, which means direct access to the system in the
test code itself (subprocesses are still available, without having to
use SSH within the test, but would become far less necessary), something
that is not possible with any current prototype. Where the
<code class="docutils literal notranslate"><span class="pre">pytest-xdist</span></code> plugin copies the package of code to the target node
and runs it, the pytest-lisa plugin could instantiate that node (boot
the necessary image on a remote machine or launch a new Hyper-V or Azure
VM, etc.) for the tests.</p>
<p>However, this use of pytest-dist requires full Python support on the
target machines, and drastically changes how developers write tests.
Furthermore, it would not support running local commands against the
remote node (like ping) or running the test across a reboot of the node.
Thus we do not want to use this functionality of <code class="docutils literal notranslate"><span class="pre">pytest-xdist</span></code>. That
said, <code class="docutils literal notranslate"><span class="pre">pytest-xdist</span></code> will still be useful for running tests
concurrently, as described above.</p>
</div>
<div class="section" id="using-paramiko-instead-of-fabric">
<h3>Using Paramiko Instead of Fabric<a class="headerlink" href="#using-paramiko-instead-of-fabric" title="Permalink to this headline">¶</a></h3>
<p>The Paramiko library is less complex (smaller library footprint) than
Fabric, as the latter wraps the former, but it is a bit more difficult
to use, and doesn’t support reading existing SSH config files, nor does
it support “ProxyJump” which we use heavily. Fabric instead provides a
clean high-level interface for existing shell commands, handling all the
connection abstractions for us.</p>
<p>Using Paramiko looked like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>

<span class="kn">from</span> <span class="nn">paramiko</span> <span class="kn">import</span> <span class="n">SSHClient</span>

<span class="kn">import</span> <span class="nn">pytest</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span> <span class="nf">node</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">SSHClient</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">SSHClient</span><span class="p">()</span> <span class="k">as</span> <span class="n">client</span><span class="p">:</span>
        <span class="n">client</span><span class="o">.</span><span class="n">load_system_host_keys</span><span class="p">()</span>
        <span class="n">client</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">hostname</span><span class="o">=</span><span class="s2">&quot;...&quot;</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">client</span>


<span class="k">def</span> <span class="nf">test_lis_version</span><span class="p">(</span><span class="n">node</span><span class="p">:</span> <span class="n">SSHClient</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">node</span><span class="o">.</span><span class="n">open_sftp</span><span class="p">()</span> <span class="k">as</span> <span class="n">sftp</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;utils.sh&quot;</span><span class="p">,</span> <span class="s2">&quot;LIS-VERSION-CHECK.sh&quot;</span><span class="p">]:</span>
            <span class="n">sftp</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">LINUX_SCRIPTS</span> <span class="o">/</span> <span class="n">f</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">stdout</span><span class="p">,</span> <span class="n">stderr</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">exec_command</span><span class="p">(</span><span class="s2">&quot;./LIS-VERSION-CHECK.sh&quot;</span><span class="p">)</span>
        <span class="n">sftp</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;state.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;state.txt&quot;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;state.txt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">open</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">f</span><span class="o">.</span><span class="n">readline</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;TestCompleted&quot;</span>
</pre></div>
</div>
<p>It is more verbose than necessary when compared to Fabric.</p>
</div>
<div class="section" id="stringio">
<h3>StringIO<a class="headerlink" href="#stringio" title="Permalink to this headline">¶</a></h3>
<p>For <code class="docutils literal notranslate"><span class="pre">Node.cat()</span></code> it would seem we could use <code class="docutils literal notranslate"><span class="pre">StringIO</span></code> like so:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="n">StringIO</span>

<span class="k">with</span> <span class="n">StringIO</span><span class="p">()</span> <span class="k">as</span> <span class="n">result</span><span class="p">:</span>
    <span class="n">node</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;state.txt&quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">result</span><span class="o">.</span><span class="n">getvalue</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;TestCompleted&quot;</span>
</pre></div>
</div>
<p>However, the data returned by Paramiko is in bytes, which in Python 3
are not equivalent to strings, hence the existing implementation which
uses <code class="docutils literal notranslate"><span class="pre">BytesIO</span></code> and decodes the bytes to a string.</p>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">LISA</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="USAGE.html">Usage</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Design</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#why-pytest">Why Pytest?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-are-we-maintaining">What are we maintaining?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pytest-target">pytest-target</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#how-are-targets-provided-and-accessed">How are targets provided and accessed?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#whats-the-target-class">What’s the <code class="xref py py-class docutils literal notranslate"><span class="pre">Target</span></code> class?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-do-we-interact-with-azure">How do we interact with Azure?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-are-targets-requested-and-managed">How are targets requested and managed?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pytest-lisa">pytest-lisa</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#how-are-tests-described">How are tests described?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-are-tests-selected">How are tests selected?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-are-results-reported">How are results reported?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-is-setup-run-and-cleanup-handled">How is setup, run, and cleanup handled?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-are-tests-timed-out">How are tests timed out?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-are-tests-organized">How are tests organized?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-will-we-port-lisav2-tests">How will we port LISAv2 tests?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-are-tests-and-functions-retried">How are tests and functions retried?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-are-tests-executed-in-parallel">How are tests executed in parallel?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#what-are-the-user-modes">What are the user modes?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pytest-playbook">pytest-playbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-does-the-flow-of-pytest-look-like">What does the “flow” of Pytest look like?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-else">What Else?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-alternatives-were-tried">What alternatives were tried?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#writing-another-framework">Writing Another Framework</a></li>
<li class="toctree-l3"><a class="reference internal" href="#using-remote-capabilities-of-pytest-xdist">Using Remote Capabilities of <code class="docutils literal notranslate"><span class="pre">pytest-xdist</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#using-paramiko-instead-of-fabric">Using Paramiko Instead of Fabric</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stringio">StringIO</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="CONTRIBUTING.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="CODE_OF_CONDUCT.html">Code of Conduct</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules/lisa.html">lisa</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/target.html">target</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/playbook.html">playbook</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="USAGE.html" title="previous chapter">How to Use Pytest and LISA</a></li>
      <li>Next: <a href="CONTRIBUTING.html" title="next chapter">Contributing Guidelines</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021 Microsoft Corporation.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.4.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/DESIGN.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>